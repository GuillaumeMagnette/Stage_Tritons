{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Triplet_Loss_Newts_ReID.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3rc1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuillaumeMagnette/Stage_Tritons/blob/master/Triplet_Loss_Newts_ReID.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "tuOe1ymfHZPu",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "# TensorFlow Addons Losses: TripletSemiHardLoss\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/addons/tutorials/losses_triplet\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/addons/blob/master/docs/tutorials/losses_triplet.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/addons/blob/master/docs/tutorials/losses_triplet.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/addons/docs/tutorials/losses_triplet.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Overview\n",
        "This notebook will demonstrate how to use the TripletSemiHardLoss function in TensorFlow Addons.\n",
        "\n",
        "### Resources:\n",
        "* [FaceNet:  A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf)\n",
        "* [Oliver Moindrot's blog does an excellent job of describing the algorithm in detail](https://omoindrot.github.io/triplet-loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bQwBbFVAyHJ_"
      },
      "source": [
        "## TripletLoss\n",
        "\n",
        "As first introduced in the FaceNet paper, TripletLoss is a loss function that trains a neural network to closely embed features of the same class while maximizing the distance between embeddings of different classes.  To do this an anchor  is chosen along with one negative and one positive sample.\n",
        "![fig3](https://user-images.githubusercontent.com/18154355/61485418-1cbb1f00-a96f-11e9-8de8-3c46eef5a7dc.png)\n",
        "\n",
        "**The loss function is described as a Euclidean distance function:**\n",
        "\n",
        "![function](https://user-images.githubusercontent.com/18154355/61484709-7589b800-a96d-11e9-9c3c-e880514af4b7.png)\n",
        "\n",
        "Where A is our anchor input,  P is the positive sample input,  N is the negative sample input, and alpha is some margin we use to specify when a triplet has become too \"easy\" and we no longer want to adjust the weights from it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wPJ5521HZHeL"
      },
      "source": [
        "## SemiHard Online Learning\n",
        "As shown in the paper, the best results are from triplets known as \"Semi-Hard\". These are defined as triplets where the negative is farther from the anchor than the positive, but still produces a positive loss. To efficiently find these triplets we utilize online learning and only train from the Semi-Hard examples in each batch. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBAlye9Q4If4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install git+https://github.com/tensorflow/docs\n",
        "#!git clone https://github.com/GuillaumeMagnette/Stage_Tritons.git\n",
        "#!pip install numba\n",
        "\n",
        "\n",
        "import urllib\n",
        "import tensorflow_addons as tfa\n",
        "#import tensorflow_docs as tfdocs\n",
        "#import tensorflow_docs.plots\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import PIL.Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.figsize'] = (12, 5)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyGX3Fd8O_qr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import datasets\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.preprocessing.image import save_img\n",
        "\n",
        "from numpy import linalg as LA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4omnMszWAKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.config.experimental.list_physical_devices('GPU')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJfSSZiNGLE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "import IPython.display as display\n",
        "from PIL import Image\n",
        "import os\n",
        "import pathlib\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from google.colab import drive\n",
        "!rm -rf /content/content\n",
        "drive.mount('/content/drive')\n",
        "#!cp '/content/drive/My Drive/slim.zip' slim.zip\n",
        "!unzip -q /content/drive/\"My Drive\"/Stage/Stage_cropped.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R4I0v6xI5M1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from skimage.io import imread, imsave\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import transform\n",
        "from skimage.transform import rotate, AffineTransform, swirl\n",
        "from skimage.util import random_noise\n",
        "from skimage.filters import gaussian\n",
        "from scipy import ndimage\n",
        "\n",
        "\n",
        "import numba\n",
        "from numba import jit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0_D7CZqkv_Hj"
      },
      "source": [
        "## Prepare the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9db_8jDFoyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import random, uniform\n",
        "\n",
        "def auto_canny(image, sigma=0.33):\n",
        "    # compute the median of the single channel pixel intensities\n",
        "    v = np.median(image)\n",
        "\n",
        "    # apply automatic Canny edge detection using the computed median\n",
        "    lower = int(max(0, (1.0 - sigma) * v))\n",
        "    upper = int(min(255, (1.0 + sigma) * v))\n",
        "    edged = cv2.Canny(image, lower, upper)\n",
        "\n",
        "    # return the edged image\n",
        "    return edged\n",
        "\n",
        "\n",
        "def preprocess_newts(img_path, input_shape):\n",
        "  #img = imread(img_path)\n",
        "  img = cv2.imread(img_path)\n",
        "# plot original Image\n",
        "#plt.imshow(img)\n",
        "#plt.show()\n",
        "#cv2_imshow(img)\n",
        "\n",
        "\n",
        "  h,w=img.shape[0:2]\n",
        "\n",
        "### Add borders around the image to have more room for wraping and swirling\n",
        "  shift = 20\n",
        "  base_size=h+2*shift,w+2*shift,3\n",
        "# make a 3 channel image for base which is slightly larger than target img\n",
        "  base=np.zeros(base_size)\n",
        "  cv2.rectangle(base,(0,0),(w+2*shift,h+2*shift),(255,255,255),40) # really thick white rectangle\n",
        "  base[shift:h+shift,shift:w+shift]=img # this works\n",
        "  img = base\n",
        "#print(img)\n",
        "#cv2_imshow(img)\n",
        "\n",
        "  h,w=img.shape[0:2]\n",
        "\n",
        "  #print(\"Augmentation par déformation\")\n",
        "\n",
        "### image shearing using sklearn.transform.AffineTransform\n",
        "# try out with differnt values of shear \n",
        "\n",
        "  shear = uniform(-.3, .3)\n",
        "  #print(shear)\n",
        "  tfr = AffineTransform(shear=shear)\n",
        "  sheared = transform.warp(img, tfr, order=1, preserve_range=True,mode='constant', cval=255)\n",
        "#sheared_fig = plot_side_by_side(img, sheared, 'Original', 'Sheared')\n",
        "#plt.title('shear')\n",
        "#plt.imshow(sheared)\n",
        "#v2_imshow(sheared)\n",
        "\n",
        "### image swirling using sklearn swirl\n",
        "# keep strength  between -1 and 1\n",
        "# maximum radius = 500\n",
        "# mode = constant because in case of \"wrap\" some parts of the newts go the other sides of the image. Bad for the training of the neural network\n",
        "  strength = uniform(-1, 1)\n",
        "#print(strength)\n",
        "  swirled = swirl(sheared, rotation=0, strength=strength, radius=500,mode='constant', cval=255)\n",
        "#plt.imshow(swirled)\n",
        "#cv2_imshow(swirled)\n",
        "\n",
        "  img = swirled\n",
        "  #print(img.shape)\n",
        "  img = img.astype(np.uint8)\n",
        "\n",
        "  img0 = cv2.equalizeHist(img[:,:,0])\n",
        "  img1 = cv2.equalizeHist(img[:,:,1])\n",
        "  img2 = cv2.equalizeHist(img[:,:,2])\n",
        "  img[:,:,0] = img0\n",
        "  img[:,:,1] = img1\n",
        "  img[:,:,2] = img2\n",
        "\n",
        "\n",
        "#cv2_imshow(img)\n",
        "\n",
        "  h,w=img.shape[0:2]\n",
        "\n",
        "\n",
        "\n",
        "  hls = cv2.cvtColor(img, cv2.COLOR_BGR2HLS) \n",
        "  lower_yellow = np.array([5,15,25]) \n",
        "  upper_yellow = np.array([100,240,255]) \n",
        "\n",
        "  lower_yellow2 = np.array([160,15,25]) \n",
        "  upper_yellow2 = np.array([180,240,255]) \n",
        "#hsl(63, 20%, 81%)\n",
        "  lower_black = np.array([0,0,50]) \n",
        "  upper_black = np.array([35,75,255]) \n",
        "\n",
        "  lower_black2 = np.array([155,0,50]) \n",
        "  upper_black2 = np.array([180,75,255]) \n",
        "# Here we are defining range of yellow color in HSL \n",
        "# This creates a mask of yellow coloured  \n",
        "# objects found in the frame. \n",
        "  mask = cv2.inRange(hls, lower_yellow, upper_yellow)\n",
        "  mask2 = cv2.inRange(hls, lower_yellow2, upper_yellow2)\n",
        "  mask_b = cv2.inRange(hls, lower_black, upper_black) \n",
        "  mask_b2 = cv2.inRange(hls, lower_black2, upper_black2)\n",
        "\n",
        "  mask = mask_b2+mask_b+mask+mask2\n",
        "# The bitwise and of the frame and mask is done so  \n",
        "# that only the blue coloured objects are highlighted  \n",
        "# and stored in res \n",
        "#res = cv2.bitwise_or()\n",
        "  res = cv2.bitwise_and(img,img, mask= mask) \n",
        "#cv2_imshow(img) \n",
        "#cv2_imshow(mask)\n",
        "#cv2_imshow(res) \n",
        "\n",
        "\n",
        "  mask_blur = cv2.GaussianBlur(mask,(5,5),cv2.BORDER_DEFAULT)\n",
        "\n",
        "  threshMap = cv2.threshold(mask_blur.astype(\"uint8\"), 0, 255,\n",
        "\t  cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
        "\n",
        "  kernel = np.ones((7,7),np.uint8)\n",
        "  threshMap = cv2.dilate(threshMap, kernel)\n",
        "  threshMap = cv2.erode(threshMap, kernel)\n",
        "\n",
        "\n",
        "#cv2_imshow(mask_blur)\n",
        "#cv2_imshow(threshMap)\n",
        "\n",
        "  canny_output = auto_canny(threshMap, sigma = 0.3)\n",
        "#canny_output = cv2.convertScaleAbs(canny_output)\n",
        "  kernel = np.ones((9,9),np.uint8)\n",
        "  threshed = cv2.dilate(canny_output,kernel)\n",
        "\n",
        "\n",
        "#print(canny_output[1])\n",
        "#plt.imshow(canny_output)\n",
        "#plt.colorbar()\n",
        "#plt.show()\n",
        "\n",
        "\n",
        "## findContours(查找轮廓)\n",
        "  cnts = cv2.findContours(threshed, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[-2]\n",
        "\n",
        "#print(cnts[0])\n",
        "#plt.imshow(threshed)\n",
        "#plt.colorbar()\n",
        "#plt.show()\n",
        "#print(cnts.shape)\n",
        "\n",
        "#new,contours, hierarchy = cv2.findContours(threshed, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "#contours= sorted(contours, key = cv2.contourArea, reverse = True)[:4]\n",
        "#c=contours[0]\n",
        "#print(cv2.contourArea(c))\n",
        "#final = cv2.drawContours(img, [c], -1, (255,0, 0), 3)\n",
        "\n",
        "\n",
        "#cnts = canny_output\n",
        "#cnts = sorted(canny_output, key=cv2.contourArea)\n",
        "## sorted by area(按照面积排序)\n",
        "  cnts = sorted(cnts, key=cv2.contourArea)\n",
        "\n",
        "## get the maximum's boundinRect(获取最大边缘的外接矩形)\n",
        "  cnt = cnts[-1]\n",
        "\n",
        "## create mask(创建掩模)\n",
        "  mask = np.ones_like(threshMap, np.uint8)*cv2.GC_PR_BGD\n",
        "  cv2.drawContours(mask, cnt, -1, cv2.GC_FGD, -1)\n",
        "\n",
        "#print(\"shape of cnt: {}\".format(cnt.shape))\n",
        "\n",
        "#plt.imshow(mask)\n",
        "#plt.colorbar()\n",
        "#plt.show()\n",
        "\n",
        "#new_image = cv2.bitwise_and(img,img,mask=mask)\n",
        "#print(mask)\n",
        "  new_image = threshMap.copy()\n",
        "  new_image[mask == 2] = 0  # Set values not masked to be 0\n",
        "\n",
        "#plt.imshow(new_image)\n",
        "#plt.colorbar()\n",
        "#plt.show()\n",
        "\n",
        "#retval = cv2.contourArea(cnt)\n",
        "#print(retval)\n",
        "\n",
        "  rect = cv2.minAreaRect(cnt)\n",
        "  #print(\"rect: {}\".format(rect))\n",
        "\n",
        "  box = cv2.boxPoints(rect)\n",
        "  box = np.int0(box)\n",
        "\n",
        "  width = int(rect[1][0])\n",
        "  height = int(rect[1][1])\n",
        "\n",
        "  src_pts = box.astype(\"float32\")\n",
        "  dst_pts = np.array([[0, height-1],\n",
        "                    [0, 0],\n",
        "                    [width-1, 0],\n",
        "                    [width-1, height-1]], dtype=\"float32\")\n",
        "  M = cv2.getPerspectiveTransform(src_pts, dst_pts)\n",
        "  warped = cv2.warpPerspective(img, M, (width, height))\n",
        "\n",
        "#cv2_imshow(warped)\n",
        "\n",
        "  #print(\"Augmentation par couleurs\")\n",
        "  image = tf.image.random_brightness(warped, max_delta=0.15) # Random brightness\n",
        "  image = tf.image.random_flip_left_right(image)\n",
        "  image = tf.image.random_flip_up_down(image)\n",
        "  image = tf.image.random_contrast(image,0.5,2)\n",
        "  image = tf.image.random_saturation(image,0.5,2)\n",
        "\n",
        "  height = input_shape[0]\n",
        "  width = input_shape[1]\n",
        "  \n",
        "  if (image.shape[0] < image.shape[1]):\n",
        "    image = tf.image.resize(image, [width, height])\n",
        "    #print(\"height < width\")\n",
        "    image = tf.image.transpose(image)\n",
        "  else:\n",
        "    image = tf.image.resize(image, [height, width])\n",
        "    #print(\"height > width\")\n",
        "\n",
        "  return image\n",
        "\n",
        "#from keras.preprocessing.image import array_to_img, img_to_array\n",
        "#img = img_to_array(image)\n",
        "#cv2_imshow(img.astype('uint8'))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsvKhAwzSPx6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape = (100,100,3)\n",
        "\n",
        "\n",
        "def augmentationImage(img_path,img_aug_path, input_shape):\n",
        "\n",
        "  \n",
        "  !rm -rf /content/databaseAug\n",
        "  data = pd.read_csv('/content/training.csv',)\n",
        "  #data = data.to_numpy()\n",
        "  labels = data.iloc[:,4]\n",
        "  #print(labels.head())\n",
        "  names = data.iloc[:,0]\n",
        "  #print(names.head())\n",
        "  #abels.head()\n",
        "  labels_uniques, counts = np.unique(labels, return_counts=True)\n",
        "  \n",
        "  #print(labels_uniques)\n",
        "  for label in labels_uniques:\n",
        "    os.makedirs(img_aug_path+'/train/'+label)\n",
        "    os.makedirs(img_aug_path+'/test/'+label)\n",
        "\n",
        "\n",
        "  print(\"Création de la base de données augmentées en cours...\")\n",
        "  # List all files in a directory using scandir()\n",
        "  basepath = '/content/database'\n",
        "  with os.scandir(basepath) as images:\n",
        "    for im in images:\n",
        "      #print(im.name)\n",
        "      if(im.name in names.values):\n",
        "        \n",
        "        id = names[names == im.name].index[0]\n",
        "        #id = names.loc[im.name]\n",
        "        #print(id)\n",
        "        label = labels.iloc[id]\n",
        "        #print('label : '+label)\n",
        "        id_label = np.where(labels_uniques == label)\n",
        "        #print(id_label)\n",
        "        imagePath = basepath + '/' + im.name\n",
        "        #print(imagePath)\n",
        "        \n",
        "        count = counts[id_label]\n",
        "        #print(count)\n",
        "        #img_path = '/content/database/Bascha_P01_T01_K14_M_Adult_1024_20190605213228.jpg'\n",
        "        #img = prepare_img(input_shape,imagePath)\n",
        "        #image = cv2_imread(path + '/' + im.name)\n",
        "        \n",
        "        \n",
        "        k = 0\n",
        "        while (k < 20):  ######### DIVISER PAR COUNT QUAND BASE DE DONNEE COMPLETE\n",
        "          #image = tf.image.random_brightness(img, max_delta=0.15) # Random brightness\n",
        "          #image = tf.image.random_flip_left_right(image)\n",
        "          #image = tf.image.random_flip_up_down(image)\n",
        "          #image = tf.image.random_contrast(image,0.5,3)\n",
        "          #image = tf.image.random_saturation(image,0.5,3)\n",
        "          image = preprocess_newts(imagePath, input_shape)\n",
        "         \n",
        "          augPath = img_aug_path+'/train/'+label+'/' + im.name[:-4] + str(k) + '.jpg'\n",
        "          #augPath = img_aug_path+'/test/'+label+'/' + im.name + str(k)\n",
        "          save_img(augPath, image)\n",
        "\n",
        "          k += 1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lg58GZPqNerv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_path = '/content/database'\n",
        "img_aug_path = '/content/databaseAug'\n",
        "augmentationImage(img_path,img_aug_path,input_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5UMFueHCgvKM"
      },
      "source": [
        "Séparation en train et test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-2EsJ7SFgvKM",
        "colab": {}
      },
      "source": [
        "\n",
        "train_path = '/content/databaseAug/train'\n",
        "test_path = '/content/databaseAug/test'\n",
        "dest1 =  pathlib.Path(test_path)\n",
        "source1 = pathlib.Path(train_path)\n",
        "\n",
        "for label in os.listdir(train_path):\n",
        "  if len(os.listdir(train_path + '/' + label)) == 0: # Check if empty..\n",
        "    shutil.rmtree(train_path + '/' + label) # Delete..\n",
        "\n",
        "import fnmatch\n",
        "\n",
        "\n",
        "\n",
        "for class_name in os.listdir(train_path):\n",
        "      \n",
        "\n",
        "      class_path = os.path.join(source1, class_name)\n",
        "      class_path_test = os.path.join(dest1, class_name)\n",
        "      #os.mkdir(class_path_test)\n",
        "      #label_map_dict[class_name]=count_label\n",
        "      imgs = fnmatch.filter(os.listdir(class_path), '*.jpg')\n",
        "      nbr_img = len(imgs)\n",
        "\n",
        "      idx = np.arange(nbr_img)\n",
        "      idx_test = np.random.choice(idx, size=10, replace=False)\n",
        "      print(len(os.listdir(class_path)))\n",
        "\n",
        "      #Copie de certains éléments dans le test set\n",
        "      for i in range (len(os.listdir(class_path))):\n",
        "        if i in idx_test:\n",
        "          print(str(class_path) + '/'+ str(os.listdir(class_path)[i]))\n",
        "          shutil.copy(str(class_path) + '/'+ str(os.listdir(class_path)[i]), str(class_path_test) + '/'+ str(os.listdir(class_path)[i]))\n",
        "      #Suppression des éléments copiés dans le train set\n",
        "      files = os.listdir(class_path)\n",
        "      for i in range (len(files)):\n",
        "        if i in idx_test:\n",
        "          print(str(class_path) + '/'+ str(files[i]))\n",
        "          file_path = pathlib.Path(str(class_path) + '/'+ str(files[i]))\n",
        "          os.remove(file_path)\n",
        "          #print(\"removed\")\n",
        "\n",
        "\n",
        "for label in os.listdir(test_path):\n",
        "  if len(os.listdir(test_path + '/' + label)) == 0: # Check if empty..\n",
        "    shutil.rmtree(test_path + '/' + label) # Delete.."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVBpICWkNi4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_size = sum(len(files) for _, _, files in os.walk(r'/content/databaseAug/train'))\n",
        "print(train_size)\n",
        "test_size = sum(len(files) for _, _, files in os.walk(r'/content/databaseAug/test'))\n",
        "print(test_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unt19KTVHOkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "#width = 56\n",
        "#height = 56\n",
        "\n",
        "train_path = '/content/databaseAug/train'\n",
        "test_path = '/content/databaseAug/test'\n",
        "\n",
        "def get_label(file_path):\n",
        "  # convert the path to a list of path components\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "  print(parts[-2])\n",
        "  # The second to last is the class-directory\n",
        "  return parts[-2] == CLASS_NAMES\n",
        "\n",
        "def decode_img(img):\n",
        "  # convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "  # resize the image to the desired size.\n",
        "  return img\n",
        "\n",
        "def process_path(file_path):\n",
        "  #print(file_path)\n",
        "  label = get_label(file_path)\n",
        "  # load the raw data from the file as a string\n",
        "  img = tf.io.read_file(file_path)\n",
        "  try:\n",
        "    img = decode_img(img)\n",
        "    print(\"decoded\")\n",
        "    counter +=1\n",
        "  except:\n",
        "    print(\"erreur décodage\")\n",
        "\n",
        "  return img, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ztNT85nNf8pj",
        "colab": {}
      },
      "source": [
        "\n",
        "#BATCH_SIZE = 32\n",
        "#input_shape = (50,35,3)\n",
        "#data_dir = \"/content/male\"\n",
        "\n",
        "train_path = '/content/databaseAug/train'\n",
        "test_path = '/content/databaseAug/test'\n",
        "\n",
        "data_dir = pathlib.Path(train_path)\n",
        "test_dir = pathlib.Path(test_path)\n",
        "\n",
        "list_ds = tf.data.Dataset.list_files(str(data_dir)+'/*/*')\n",
        "list_ds_test = tf.data.Dataset.list_files(str(test_dir)+'/*/*')\n",
        "\n",
        "CLASS_NAMES = np.array([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"])\n",
        "#print(type(CLASS_NAMES))\n",
        "print(CLASS_NAMES)\n",
        "#for f in list_ds.take(5):\n",
        "  #print(f.numpy())\n",
        "#counter = 0\n",
        "\n",
        "def show_batch(image_batch, label_batch):\n",
        "  plt.figure(figsize=(10,10))\n",
        "  for n in range(25):\n",
        "      ax = plt.subplot(5,5,n+1)\n",
        "      plt.imshow(image_batch[n])\n",
        "      plt.title(CLASS_NAMES[label_batch[n]==1][0].title())\n",
        "      plt.axis('off')\n",
        "\n",
        "def augment(image,label):\n",
        "  #image,label = convert(image, label)\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32) # Cast and normalize the image to [0,1]\n",
        "  #image = tf.image.resize_with_crop_or_pad(image, 34, 34) # Add 6 pixels of padding\n",
        "  #image = tf.image.random_crop(image, size=[28, 28, 1]) # Random crop back to 28x28\n",
        "  image = tf.image.random_brightness(image, max_delta=0.5) # Random brightness\n",
        "  image = tf.image.flip_left_right(image)\n",
        "\n",
        "  return image,label\n",
        "#Use Dataset.map to create a dataset of image, label pairs:\n",
        "\n",
        "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
        "labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "labeled_ds_test = list_ds_test.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "#train_size = int(0.7 * nbr_element)\n",
        "\n",
        "#val_size = int(0.30 * nbr_element)\n",
        "#test_size = int(0.15 * DATASET_SIZE)\n",
        "\n",
        "\n",
        "#full_dataset = labeled_ds.shuffle()\n",
        "#train_dataset = labeled_ds.take(train_size)\n",
        "#test_dataset = labeled_ds.skip(train_size)\n",
        "#val_dataset = test_dataset.skip(test_size)\n",
        "#test_dataset = test_dataset.take(val_size)\n",
        "#train_dataset = labeled_ds.map(augment, num_parallel_calls=AUTOTUNE)\n",
        "#test_dataset = labeled_ds.map(augment, num_parallel_calls=AUTOTUNE)\n",
        "count_label = len(os.listdir('/content/databaseAug/train'))\n",
        "print(count_label)\n",
        "\n",
        "x_train = np.zeros((train_size,input_shape[0],input_shape[1],3))\n",
        "y_train = np.zeros((train_size))\n",
        "x_test = np.zeros((test_size,input_shape[0],input_shape[1],3))\n",
        "y_test = np.zeros((test_size))\n",
        "\n",
        "k=0\n",
        "for image, label in labeled_ds:\n",
        "  #print(label)\n",
        "  x_train[k,:,:,:] = image\n",
        "  y_train[k] = np.where(label)[0][0]\n",
        "  k += 1\n",
        "k=0\n",
        "for image, label in labeled_ds_test:\n",
        "  x_test[k,:,:,:] = image\n",
        "  y_test[k] = np.where(label)[0][0]\n",
        "  k += 1\n",
        "  print(np.where(label)[0])\n",
        "  \n",
        "  #print(\"Image shape: \", image.numpy().shape)\n",
        "  #print(\"Label: \", label.numpy())\n",
        "#print(y_train)\n",
        "dataset = []\n",
        "dataset_test = []\n",
        "    \n",
        "#Sorting images by classes and normalize values 0=>\n",
        "for n in range(count_label):\n",
        "    images_class_n_train = np.asarray([row for idx,row in enumerate(x_train) if y_train[idx]==n])\n",
        "    dataset.append(images_class_n_train/255)\n",
        "\n",
        "\n",
        "    images_class_n_test = np.asarray([row for idx,row in enumerate(x_test) if y_test[idx]==n])\n",
        "    dataset_test.append(images_class_n_test/255)\n",
        "\n",
        "  \n",
        "print(\"nombre de classes différentes : \"+ str(count_label))\n",
        "#for n in range(int(count_label*0.30)):\n",
        " #   images_class_n = np.asarray([row for idx,row in enumerate(x_test) if y_test[idx]==n])\n",
        "    #print(images_class_n.shape)\n",
        "  #  dataset_test.append(images_class_n/255)\n",
        "        \n",
        "    #images_class_n = np.asarray([row for idx,row in enumerate(x_test_origin) if y_test_origin[idx]==n])\n",
        "    #dataset_test.append(images_class_n/255)\n",
        "#input_shape = [width,height,3]\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KR01t9v_fxbT"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wvOPPuIKhLJi"
      },
      "source": [
        "![fig2](https://user-images.githubusercontent.com/18154355/61485417-1cbb1f00-a96f-11e9-8d6a-94964ce8c4db.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8zzNXNWMv3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Lambda, BatchNormalization, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.backend import l2_normalize\n",
        "from tensorflow.keras import Input, Model\n",
        "\n",
        "def build_network(input_shape, embeddingsize):\n",
        "    '''\n",
        "    Define the neural network to learn image similarity\n",
        "    Input : \n",
        "            input_shape : shape of input images\n",
        "            embeddingsize : vectorsize used to encode our picture   \n",
        "    '''\n",
        "     # Convolutional Neural Network\n",
        "    network = Sequential()\n",
        "    network.add(Conv2D(128, (7,7), activation='relu',\n",
        "                     input_shape=input_shape,\n",
        "                     kernel_initializer='he_uniform',\n",
        "                     kernel_regularizer=l2(2e-4)))\n",
        "    network.add(MaxPooling2D())\n",
        "    network.add(Dropout(0.5))\n",
        "    network.add(BatchNormalization())\n",
        "    network.add(Conv2D(128, (5,5), activation='relu', kernel_initializer='he_uniform',\n",
        "                     kernel_regularizer=l2(2e-4)))\n",
        "    network.add(MaxPooling2D())\n",
        "    network.add(Dropout(0.5))\n",
        "    network.add(BatchNormalization())\n",
        "    network.add(Conv2D(256, (3,3), activation='relu', kernel_initializer='he_uniform',\n",
        "                     kernel_regularizer=l2(2e-4)))\n",
        "    network.add(MaxPooling2D())\n",
        "    #network.add(Dropout(0.5))\n",
        "    \n",
        "    network.add(Flatten())\n",
        "    #network.add(Dense(4096, activation='relu',\n",
        "     #              kernel_regularizer=l2(1e-3),\n",
        "      #             kernel_initializer='he_uniform'))\n",
        "    network.add(BatchNormalization())\n",
        "    \n",
        "    network.add(Dense(embeddingsize, activation=None,\n",
        "                   kernel_regularizer=l2(1e-3),\n",
        "                   kernel_initializer='he_uniform'))\n",
        "    \n",
        "    #Force the encoding to live on the d-dimentional hypershpere\n",
        "    network.add(Lambda(lambda x: l2_normalize(x,axis=-1)))\n",
        "    \n",
        "    return network\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HYE-BxhOzFQp"
      },
      "source": [
        "## Train and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NxfYhtiSzHf-",
        "colab": {}
      },
      "source": [
        "#input_shape = [100,100,3]\n",
        "model = build_network(input_shape, 128)\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    loss=tfa.losses.TripletSemiHardLoss())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TGBYNGxgVDrj",
        "colab": {}
      },
      "source": [
        "# Train the network\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Y--0tK69SXf",
        "colab": {}
      },
      "source": [
        "# Evaluate the network\n",
        "results = model.predict(test_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dqSuLdVZGNrZ",
        "colab": {}
      },
      "source": [
        "# Save test embeddings for visualization in projector\n",
        "np.savetxt(\"vecs.tsv\", results, delimiter='\\t')\n",
        "\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "for img, labels in tfds.as_numpy(test_dataset):\n",
        "    [out_m.write(str(x) + \"\\n\") for x in labels]\n",
        "out_m.close()\n",
        "\n",
        "\n",
        "try:\n",
        "  from google.colab import files\n",
        "  files.download('vecs.tsv')\n",
        "  files.download('meta.tsv')\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VAtj_m6Z_Uwe"
      },
      "source": [
        "## Embedding Projector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y4rjlG9rlbVA"
      },
      "source": [
        "The vector and metadata files can be loaded and visualized here: https://projector.tensorflow.org/\n",
        "\n",
        "You can see the results of our embedded test data when visualized with UMAP:\n",
        "![embedding](https://user-images.githubusercontent.com/18154355/61600295-e6470380-abfd-11e9-8a00-2b25e7e6916f.png)\n"
      ]
    }
  ]
}